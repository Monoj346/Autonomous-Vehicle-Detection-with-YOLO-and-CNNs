{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9288\\3015848435.py:6: DeprecationWarning: scipy.misc is deprecated and will be removed in 2.0.0\n",
      "  import scipy.misc\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "from tensorflow.keras import layers, Model\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def yolo_filter_boxes(boxes, conf, probs, thresh=0.6):\n",
    "    \"\"\"Filters YOLO boxes based on confidence and class scores.\"\"\"\n",
    "    \n",
    "    # Compute box scores\n",
    "    scores = probs * conf\n",
    "\n",
    "    # Get class with highest score & corresponding score\n",
    "    classes = tf.argmax(scores, axis=-1)\n",
    "    class_scores = tf.reduce_max(scores, axis=-1)\n",
    "\n",
    "    # Create mask for filtering\n",
    "    mask = class_scores >= thresh\n",
    "\n",
    "    # Apply mask\n",
    "    return tf.boolean_mask(class_scores, mask), tf.boolean_mask(boxes, mask), tf.boolean_mask(classes, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(b1, b2):\n",
    "    \"\"\"Compute the Intersection over Union (IoU) between two boxes.\"\"\"\n",
    "    \n",
    "    x1, y1, x2, y2 = b1\n",
    "    x1_, y1_, x2_, y2_ = b2\n",
    "\n",
    "    # Compute intersection\n",
    "    xi1, yi1 = max(x1, x1_), max(y1, y1_)\n",
    "    xi2, yi2 = min(x2, x2_), min(y2, y2_)\n",
    "    inter_w, inter_h = max(0, yi2 - yi1), max(0, xi2 - xi1)\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    # Compute union\n",
    "    area1 = (x2 - x1) * (y2 - y1)\n",
    "    area2 = (x2_ - x1_) * (y2_ - y1_)\n",
    "    union_area = area1 + area2 - inter_area\n",
    "\n",
    "    return inter_area / union_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def yolo_nms(scores, boxes, classes, max_boxes=10, iou_thresh=0.5):\n",
    "    \"\"\"Applies Non-Max Suppression (NMS) to filter boxes.\"\"\"\n",
    "    \n",
    "    max_boxes_tensor = tf.Variable(max_boxes, dtype='int32')\n",
    "\n",
    "    # Get indices of boxes to keep\n",
    "    nms_idx = tf.image.non_max_suppression(boxes, scores, max_boxes_tensor, iou_thresh)\n",
    "\n",
    "    # Select filtered scores, boxes, and classes\n",
    "    return tf.gather(scores, nms_idx), tf.gather(boxes, nms_idx), tf.gather(classes, nms_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def yolo_eval(outputs, img_shape=(720, 1280), max_boxes=10, score_thresh=0.6, iou_thresh=0.5):\n",
    "    \"\"\"Converts YOLO model output to final filtered boxes, scores, and classes.\"\"\"\n",
    "    \n",
    "    box_xy, box_wh, box_conf, box_probs = outputs\n",
    "\n",
    "    # Convert boxes to corner coordinates\n",
    "    boxes = yolo_boxes_to_corners(box_xy, box_wh)\n",
    "\n",
    "    # Apply score filtering\n",
    "    scores, boxes, classes = yolo_filter_boxes(boxes, box_conf, box_probs, score_thresh)\n",
    "\n",
    "    # Scale boxes to original image size\n",
    "    boxes = scale_boxes(boxes, img_shape)\n",
    "\n",
    "    # Apply non-max suppression\n",
    "    return yolo_nms(scores, boxes, classes, max_boxes, iou_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores[2] = 152.68243\n",
      "boxes[2] = [ 1530.0006   -3760.4392     831.5534     -19.682007]\n",
      "classes[2] = 25\n",
      "scores.shape = (10,)\n",
      "boxes.shape = (10, 4)\n",
      "classes.shape = (10,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set both TensorFlow and NumPy seeds\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "def yolo_boxes_to_corners(xy, wh):\n",
    "    \"\"\"Convert YOLO box format (x, y, w, h) to (x1, y1, x2, y2)\"\"\"\n",
    "    min_corner = xy - (wh / 2)\n",
    "    max_corner = xy + (wh / 2)\n",
    "    return tf.concat([min_corner, max_corner], axis=-1)\n",
    "\n",
    "def scale_boxes(boxes, img_shape):\n",
    "    \"\"\"Rescale the YOLO boxes to the original image size.\"\"\"\n",
    "    height, width = tf.cast(img_shape[0], tf.float32), tf.cast(img_shape[1], tf.float32)\n",
    "    scale = tf.stack([width, height, width, height])\n",
    "    return boxes * scale\n",
    "\n",
    "# Generate YOLO outputs with fixed seed\n",
    "yolo_outputs = (\n",
    "    tf.random.normal([19, 19, 5, 2], mean=1, stddev=4),\n",
    "    tf.random.normal([19, 19, 5, 2], mean=1, stddev=4),\n",
    "    tf.random.normal([19, 19, 5, 1], mean=1, stddev=4),\n",
    "    tf.random.normal([19, 19, 5, 80], mean=1, stddev=4)\n",
    ")\n",
    "\n",
    "# Run YOLO evaluation\n",
    "scores, boxes, classes = yolo_eval(yolo_outputs)\n",
    "\n",
    "# Print results\n",
    "print(\"scores[2] =\", scores[2].numpy())\n",
    "print(\"boxes[2] =\", boxes[2].numpy())\n",
    "print(\"classes[2] =\", classes[2].numpy())\n",
    "print(\"scores.shape =\", scores.numpy().shape)\n",
    "print(\"boxes.shape =\", boxes.numpy().shape)\n",
    "print(\"classes.shape =\", classes.numpy().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"YOLO\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"YOLO\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Darknet53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,568,576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,719,616</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">87,125</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m416\u001b[0m, \u001b[38;5;34m416\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Darknet53 (\u001b[38;5;33mFunctional\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m1,568,576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │     \u001b[38;5;34m4,719,616\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m85\u001b[0m)     │        \u001b[38;5;34m87,125\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,375,317</span> (24.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,375,317\u001b[0m (24.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,375,317</span> (24.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,375,317\u001b[0m (24.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def darknet53():\n",
    "    inputs = layers.Input(shape=(416, 416, 3))\n",
    "    x = layers.Conv2D(32, (3,3), padding='same', activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D(2,2)(x)\n",
    "    x = layers.Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(2,2)(x)\n",
    "    x = layers.Conv2D(128, (3,3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(2,2)(x)\n",
    "    x = layers.Conv2D(256, (3,3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(2,2)(x)\n",
    "    x = layers.Conv2D(512, (3,3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(2,2)(x)\n",
    "    return Model(inputs, x, name='Darknet53')\n",
    "\n",
    "def yolo_head(x):\n",
    "    x = layers.Conv2D(1024, (3,3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(5 + 80, (1,1), padding='same', activation='sigmoid')(x)  # Adjust output for classes\n",
    "    return x\n",
    "\n",
    "def yolo_model():\n",
    "    inputs = layers.Input(shape=(416, 416, 3))\n",
    "    darknet = darknet53()(inputs)\n",
    "    outputs = yolo_head(darknet)\n",
    "    return Model(inputs, outputs, name='YOLO')\n",
    "\n",
    "# Create the model\n",
    "model = yolo_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (416, 416))\n",
    "    image = image / 255.0  # Normalize to [0,1]\n",
    "    return np.expand_dims(image, axis=0)\n",
    "\n",
    "# --- 5️⃣ Load Dataset (Image & Labels) ---\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Loads an image, resizes it, and normalizes it.\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: Image file '{image_path}' does not exist!\")\n",
    "        return None\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not read image '{image_path}'. Skipping...\")\n",
    "        return None\n",
    "\n",
    "    image = cv2.resize(image, (416, 416))\n",
    "    image = image / 255.0  # Normalize to [0,1]\n",
    "    return image\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "def load_data(image_dir, label_dir, subset_size=50):\n",
    "    \"\"\" Load a reduced dataset with a subset of images and labels. \"\"\"\n",
    "    \n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "    random.shuffle(image_files)  # Shuffle for randomness\n",
    "\n",
    "    # Select only a subset of images\n",
    "    image_files = image_files[:subset_size]\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        label_path = os.path.join(label_dir, img_file.replace('.jpg', '.txt'))\n",
    "        \n",
    "        # Read and preprocess image\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (320, 320))  # Resize to save memory\n",
    "        img = img / 255.0  # Normalize\n",
    "        \n",
    "        images.append(img)\n",
    "\n",
    "        # Read label file\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                label_data = f.readlines()\n",
    "            labels.append(label_data)\n",
    "        else:\n",
    "            labels.append([])  # Empty label for missing files\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Load a smaller dataset**\n",
    "images, labels = load_data(\n",
    "    r\"C:\\Users\\user\\Downloads\\archive\\vehicle dataset\\train\\images\",\n",
    "    r\"C:\\Users\\user\\Downloads\\archive\\vehicle dataset\\train\\labels\",\n",
    "    subset_size=50  # Adjust this number as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def yolo_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function for YOLO.\n",
    "    \"\"\"\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    ce = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    # Extract values from predictions\n",
    "    pred_xy = y_pred[..., :2]\n",
    "    pred_wh = y_pred[..., 2:4]\n",
    "    pred_conf = y_pred[..., 4:5]\n",
    "    pred_class = y_pred[..., 5:]\n",
    "\n",
    "    # Extract ground truth values\n",
    "    true_xy = y_true[..., :2]\n",
    "    true_wh = y_true[..., 2:4]\n",
    "    true_conf = y_true[..., 4:5]\n",
    "    true_class = y_true[..., 5:]\n",
    "\n",
    "    # Compute losses\n",
    "    bbox_loss = mse(true_xy, pred_xy) + mse(true_wh, pred_wh)\n",
    "    conf_loss = mse(true_conf, pred_conf)\n",
    "    class_loss = ce(true_class, pred_class)\n",
    "\n",
    "    total_loss = bbox_loss + conf_loss + class_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(image_paths, labels, batch_size=16):\n",
    "    while True:\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for j in range(i, min(i + batch_size, len(image_paths))):\n",
    "                img_path = image_paths[j]\n",
    "                \n",
    "                # **DEBUGGING CHECK**\n",
    "                if not isinstance(img_path, str):\n",
    "                    print(f\"Skipping invalid path at index {j}: {img_path}\")\n",
    "                    continue\n",
    "                \n",
    "                img = cv2.imread(img_path)  # Read image\n",
    "                \n",
    "                # **CHECK IF IMAGE LOADED CORRECTLY**\n",
    "                if img is None:\n",
    "                    print(f\"Error loading image: {img_path}\")\n",
    "                    continue\n",
    "                \n",
    "                img = cv2.resize(img, (416, 416))\n",
    "                img = img / 255.0  # Normalize\n",
    "                batch_images.append(img)\n",
    "                batch_labels.append(labels[j])\n",
    "            \n",
    "            if batch_images:  # Ensure batch is not empty\n",
    "                yield np.array(batch_images, dtype=np.float32), np.array(batch_labels, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m batch_size = \u001b[32m16\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_gen = data_generator(\u001b[43mimages\u001b[49m, labels, batch_size)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Compile YOLO Model\u001b[39;00m\n\u001b[32m      5\u001b[39m model.compile(optimizer=\u001b[33m\"\u001b[39m\u001b[33madam\u001b[39m\u001b[33m\"\u001b[39m, loss=\u001b[33m\"\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m\"\u001b[39m, metrics=[\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_gen = data_generator(images, labels, batch_size)\n",
    "\n",
    "# Compile YOLO Model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(train_gen, steps_per_epoch=len(images) // batch_size, epochs=10)\n",
    "\n",
    "# Save Model\n",
    "model.save(\"yolo_trained_model.h5\")\n",
    "\n",
    "# Evaluate Model Performance\n",
    "loss, accuracy = model.evaluate(train_gen, steps=len(images) // batch_size)\n",
    "print(f\"Final Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
